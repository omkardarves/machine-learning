{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sky', 'is', 'bright', '.', 'Birds', 'are', 'gone', '.', 'Nests', 'are', 'empty', '.', 'we', 'can', 'find', 'these', 'lines', 'on', 'http', ':', '//www.google.com']\n",
      "\n",
      "\n",
      "Number of Words:  21\n"
     ]
    }
   ],
   "source": [
    "FileName = (\"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\")\n",
    "\n",
    "with open(FileName, 'r') as file:\n",
    "    lines_in_file = file.read()\n",
    "    nltk_tokens = nltk.word_tokenize(lines_in_file)\n",
    "    print(nltk_tokens)\n",
    "    print(\"\\n\")\n",
    "    print(\"Number of Words: \" , len(nltk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sky', 'is', 'bright.', 'Birds', 'are', 'gone.', 'Nests', 'are', 'empty.', 'we', 'can', 'find', 'these', 'lines', 'on', 'http://www.google.com']\n",
      "\n",
      "\n",
      "Number of Words:  16\n"
     ]
    }
   ],
   "source": [
    "FileName = (\"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\")\n",
    "\n",
    "with open(FileName, 'r') as file:\n",
    "    lines_in_file = file.read()\n",
    "\n",
    "    print(lines_in_file.split())\n",
    "    print(\"\\n\")\n",
    "    print (\"Number of Words: \", len(lines_in_file.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0c9be4b32924>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Converting binary to ascii\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata_b2a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb2a_uu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"**Binary to Ascii** \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_b2a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "import binascii\n",
    "\n",
    "text = \"Sahu Technology\"\n",
    "\n",
    "# Converting binary to ascii\n",
    "data_b2a = binascii.b2a_uu(text)\n",
    "print(\"**Binary to Ascii** \\n\")\n",
    "print(data_b2a)\n",
    "\n",
    "# Converting back from ascii to binary \n",
    "data_a2b = binascii.a2b_uu(data_b2a)\n",
    "print(\"**Ascii to Binary** \\n\")\n",
    "print(data_a2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-50a9815f77e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinascii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb2a_uu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hello'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "import binascii\n",
    "bin(int(binascii.b2a_uu('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sky is bright.\n",
      "\n",
      "1\n",
      "Birds are gone.\n",
      "\n",
      "2\n",
      "Nests are empty.\n",
      "\n",
      "3\n",
      "we can find these lines on http://www.google.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open (\"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\", \"r\") as File:\n",
    "    data=File.readlines()\n",
    "\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        print(i) \n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sky is bright. Birds are gone. Nests are empty. we can find these lines on http://www.google.com \n"
     ]
    }
   ],
   "source": [
    "with open (\"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\", \"r\") as File:\n",
    "    data=File.read().replace('\\n', ' ')\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sky is bright.\n",
      "\n",
      "1\n",
      "Birds are gone.\n",
      "\n",
      "2\n",
      "Nests are empty.\n",
      "\n",
      "3\n",
      "we can find these lines on http://www.google.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open (\"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\", \"r\") as File:\n",
    "    data=File.readlines()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        print(i) \n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can find these lines on http://www.google.com\n",
      "Nests are empty.\n",
      "Birds are gone.\n",
      "Sky is bright.\n"
     ]
    }
   ],
   "source": [
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "with FileReadBackwards(\n",
    "    \"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\",\n",
    "    encoding=\"utf-8\") as File:\n",
    "\n",
    "    for line in File:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//www.google.com', ':', 'http', 'on', 'lines', 'these', 'find', 'can', 'we']\n",
      "['.', 'empty', 'are', 'Nests']\n",
      "['.', 'gone', 'are', 'Birds']\n",
      "['.', 'bright', 'is', 'Sky']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "with FileReadBackwards(\n",
    "    \"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\",\n",
    "    encoding=\"utf-8\") as File:\n",
    "\n",
    "    for line in File:\n",
    "        word_data= line\n",
    "        nltk_tokens = nltk.word_tokenize(word_data)\n",
    "        nltk_tokens.reverse()\n",
    "        print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['since', 'of', 'those', 'The', 'interested', 'Ipsum', 'for', '.', 'standard', 'is', 'reproduced', '1500s', 'chunk', 'the', 'Lorem', 'used', 'below']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "word_data = \"\"\"The standard chunk of Lorem Ipsum used\n",
    "since the 1500s is reproduced below for those interested.\"\"\"\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "\n",
    "no_order = list(set(nltk_tokens))\n",
    "\n",
    "print(no_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'standard', 'chunk', 'of', 'Lorem', 'Ipsum', 'used', 'since', 'the', '1500s', 'is', 'reproduced', 'below', 'for', 'those', 'interested', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "word_data = \"\"\"The standard chunk of Lorem Ipsum used\n",
    "since the 1500s is reproduced below for those interested.\"\"\" \n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "\n",
    "ordered_tokens = set()\n",
    "result = []\n",
    "for word in nltk_tokens:\n",
    "    if word not in ordered_tokens:\n",
    "        ordered_tokens.add(word)\n",
    "        result.append(word)\n",
    "print(result)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omi27@gmail.com']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Reach us at omi27@gmail.com\"\n",
    "\n",
    "\n",
    "emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.google.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "with open(\"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\") as file:\n",
    "        for line in file:\n",
    "            urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', line)\n",
    "        print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Sneha Lohana', 'Occupation': 'Trainer', 'Topics': {'Django', 'Python', 'Machine_Learning'}}\n",
      "\n",
      "\n",
      "***With Pretty Print***\n",
      "-----------------------\n",
      "{'Name': 'Sneha '\n",
      "         'Lohana',\n",
      " 'Occupation': 'Trainer',\n",
      " 'Topics': {'Django',\n",
      "            'Machine_Learning',\n",
      "            'Python'}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "itvedant = {'Name': 'Sneha Lohana', 'Occupation': 'Trainer','Topics':{'Python','Django','Machine_Learning'}}\n",
    "\n",
    "print(itvedant)\n",
    "print(\"\\n\")\n",
    "print(\"***With Pretty Print***\")\n",
    "print(\"-----------------------\")\n",
    "pprint.pprint(itvedant,width=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'Dept': ['IT', 'Director', 'Founder'],\n",
      "  'Name': ['Sneha', 'Mayuresh', 'Swapnil'],\n",
      "  'Salary': ['1 lacs', '4 lacs', '5 lacs'],\n",
      "  'StartDate': ['1/1/2017', '9/23/2013', '11/15/2013']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "emp = {\"Name\":[\"Sneha\",\"Mayuresh\",\"Swapnil\" ],\n",
    "   \"Salary\":['1 lacs','4 lacs','5 lacs' ],   \n",
    "   \"StartDate\":[ \"1/1/2017\",\"9/23/2013\",\"11/15/2013\"],\n",
    "   \"Dept\":[ \"IT\",\"Director\",\"Founder\"] }\n",
    "\n",
    "x= pprint.pformat(emp, indent=2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Standard Chunk Of Lorem Ipsum Used\\nSince The 1500S Is Reproduced Below For Those Interested.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "a=\"\"\"The standard chunk of Lorem Ipsum used\n",
    "since the 1500s is reproduced below for those interested.\"\"\"\n",
    "a.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Standard Chunk Of Lorem Ipsum Used Since The 1500s Is Reproduced Below For Those Interested.\n"
     ]
    }
   ],
   "source": [
    "print(string.capwords(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE STANDARD CHUNK OF LOREM IPSUM USED\\nSINCE THE 1500S IS REPRODUCED BELOW FOR THOSE INTERESTED.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The saandard chunk of Lorem Ipsum used\n",
      "sjnce ahe 1500s js reproduced belob for ahose jnaeresaed.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "a = \"\"\"The standard chunk of Lorem Ipsum used\n",
    "since the 1500s is reproduced below for those interested.\"\"\"\n",
    "\n",
    "result = str.maketrans('twxi', 'abcj')\n",
    "print(a.translate(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nests are empty.', 'Birds are gone.', 'Sky is bright.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "data = \"Nests are empty. Birds are gone. Sky is bright.\"\n",
    "tokens = nltk.sent_tokenize(data)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie geht es Ihnen?', 'Gut, danke.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen?  Gut, danke.')\n",
    "print(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nests', 'are', 'empty', '.', 'Birds', 'are', 'gone', '.', 'Sky', 'is', 'bright', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "word_data = \"Nests are empty. Birds are gone. Sky is bright.\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#It will download a file with English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "print(stopwords.words()[620:650])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The various language other than English which has these stopwords are as below.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stain', 'grunge', 'bemire', 'filth', 'dirt', 'ground', 'territory', 'colly', 'dirty', 'soil', 'begrime', 'land', 'grime', 'grease'}\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"Soil\"):\n",
    "    for lm in syn.lemmas():\n",
    "             synonyms.append(lm.name())\n",
    "print(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'begrime',\n",
       " 'bemire',\n",
       " 'colly',\n",
       " 'dirt',\n",
       " 'dirty',\n",
       " 'filth',\n",
       " 'grease',\n",
       " 'grime',\n",
       " 'ground',\n",
       " 'grunge',\n",
       " 'land',\n",
       " 'soil',\n",
       " 'stain',\n",
       " 'territory'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"Soil\"):\n",
    "    for lm in syn.lemmas():\n",
    "             synonyms.append(lm.name())\n",
    "set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clean'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"Soil\"):\n",
    "    for lm in syn.lemmas():\n",
    "        if lm.antonyms():\n",
    "            antonyms.append(lm.antonyms()[0].name())\n",
    "\n",
    "print(set(antonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते, आप कैसे हैं?\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(to_lang=\"Hindi\")\n",
    "translation = translator.translate(\"hello, how are you?\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola como estas\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(from_lang=\"Hindi\",to_lang=\"Spanish\")\n",
    "translation = translator.translate(\"नमस्ते, आप कैसे हैं?\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sky was blue,forest was green,thwas was word replacement\n",
      "Sky was blue,forest was green,thwas is word replacement\n"
     ]
    }
   ],
   "source": [
    "str = \"Sky is blue,forest is green,this is word replacement\"\n",
    "print(str.replace(\"is\", \"was\"))\n",
    "print(str.replace(\"is\", \"was\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mumbai is best.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sourceline  = re.compile(\"Delhi\", re.IGNORECASE)\n",
    " \n",
    "Replacedline  = sourceline.sub(\"Mumbai\",\"Delhi is best.\")\n",
    "print(Replacedline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "{'weak', 'walk', 'flak'}\n",
      "group\n",
      "{'groin', 'grout', 'ground', 'groan', 'grown', 'group'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['let', 'us', 'wlak','on','the','groun'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "{'weak', 'walk', 'flak'}\n",
      "group\n",
      "{'groin', 'grout', 'ground', 'groan', 'grown', 'group'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['Let', 'us', 'Wlak','on','The','groun'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['locomotive', 'engine', 'locomotive_engine', 'railway_locomotive']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "res=wn.synset('locomotive.n.01').lemma_names()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the atmosphere and outer space as viewed from the earth\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "resdef = wn.synset('sky.n.01').definition()\n",
    "print(resdef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the dog barked all night']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "res_exm = wn.synset('dog.n.01').examples()\n",
    "print(res_exm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('inclined.a.02.inclined'), Lemma('horizontal.a.01.horizontal')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "res_a = wn.lemma('vertical.a.01.vertical').antonyms()\n",
    "print(res_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "fields = gutenberg.fileids()\n",
    "\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Poems by William Blake 1789]\n",
      "\n",
      " \n",
      "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
      "and THE BOOK of THEL\n",
      "\n",
      "\n",
      " SONGS OF INNOCENCE\n",
      " \n",
      " \n",
      " INTRODUCTION\n",
      " \n",
      " Piping down the valleys wild,\n",
      "   Piping songs of pleasant glee,\n",
      " On a cloud I saw a child,\n",
      "   And he laughing said to me:\n",
      " \n",
      " \"Pipe a song about a Lamb!\"\n",
      "So I piped with merry cheer.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "sample = gutenberg.raw(\"blake-poems.txt\")\n",
    "\n",
    "token = sent_tokenize(sample)\n",
    "\n",
    "for para in range(2):\n",
    "    print(token[para])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('IN')\n",
    "nltk.help.upenn_tagset('DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('serpent', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('eats', 'VBZ'),\n",
       " ('eggs', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('nest', 'JJS')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"A Python is a serpent which eats eggs from the nest\")\n",
    "tagged_text=nltk.pos_tag(text)\n",
    "tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'JJ'), ('Poems', 'NNP'), ('by', 'IN'), ('William', 'NNP'), ('Blake', 'NNP'), ('1789', 'CD'), (']', 'NNP'), ('SONGS', 'NNP'), ('OF', 'NNP'), ('INNOCENCE', 'NNP'), ('AND', 'NNP'), ('OF', 'NNP'), ('EXPERIENCE', 'NNP'), ('and', 'CC'), ('THE', 'NNP'), ('BOOK', 'NNP'), ('of', 'IN'), ('THEL', 'NNP'), ('SONGS', 'NNP'), ('OF', 'NNP'), ('INNOCENCE', 'NNP'), ('INTRODUCTION', 'NNP'), ('Piping', 'VBG'), ('down', 'RP'), ('the', 'DT'), ('valleys', 'NN'), ('wild', 'JJ'), (',', ','), ('Piping', 'NNP'), ('songs', 'NNS'), ('of', 'IN'), ('pleasant', 'JJ'), ('glee', 'NN'), (',', ','), ('On', 'IN'), ('a', 'DT'), ('cloud', 'NN'), ('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('child', 'NN'), (',', ','), ('And', 'CC'), ('he', 'PRP'), ('laughing', 'VBG'), ('said', 'VBD'), ('to', 'TO'), ('me', 'PRP'), (':', ':'), ('``', '``'), ('Pipe', 'VB'), ('a', 'DT'), ('song', 'NN'), ('about', 'IN'), ('a', 'DT'), ('Lamb', 'NN'), ('!', '.'), (\"''\", \"''\")]\n",
      "[('So', 'RB'), ('I', 'PRP'), ('piped', 'VBD'), ('with', 'IN'), ('merry', 'NNP'), ('cheer', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "sample = gutenberg.raw(\"blake-poems.txt\")\n",
    "tokenized = sent_tokenize(sample)\n",
    "for i in tokenized[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Illegal chunk pattern: {?*}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36mfromstring\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'{'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'}'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mChunkRule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'}'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'{'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tag_pattern, descr)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;34m'(?P<chunk>%s)%s'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtag_pattern2re_pattern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mChunkString\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIN_CHINK_PATTERN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, pattern)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    425\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[1;32m--> 426\u001b[1;33m                            not nested and not items))\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    815\u001b[0m                            not (del_flags & SRE_FLAG_VERBOSE))\n\u001b[1;32m--> 816\u001b[1;33m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_verbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnested\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\")\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    425\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[1;32m--> 426\u001b[1;33m                            not nested and not items))\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    650\u001b[0m                 raise source.error(\"nothing to repeat\",\n\u001b[1;32m--> 651\u001b[1;33m                                    source.tell() - here + len(this))\n\u001b[0m\u001b[0;32m    652\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_REPEATCODES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: nothing to repeat at position 10",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-34c2fafede84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m (\"flew\", \"VBD\"), (\"through\", \"IN\"),  (\"the\", \"DT\"), (\"window\", \"NN\")]\n\u001b[0;32m      5\u001b[0m \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"NP: {?*}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegexpParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, grammar, root_label, loop, trace)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1214\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_grammar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1215\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# Make sur the grammar looks like it has the right type:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36m_read_grammar\u001b[1;34m(self, grammar, root_label, trace)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[1;31m# Add the rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m             \u001b[0mrules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRegexpChunkRule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[1;31m# Record the final stage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\regexp.py\u001b[0m in \u001b[0;36mfromstring\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    397\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Illegal chunk pattern: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Illegal chunk pattern: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Illegal chunk pattern: {?*}"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = [(\"The\", \"DT\"), (\"small\", \"JJ\"), (\"red\", \"JJ\"),(\"flower\", \"NN\"), \n",
    "(\"flew\", \"VBD\"), (\"through\", \"IN\"),  (\"the\", \"DT\"), (\"window\", \"NN\")]\n",
    "grammar = \"NP: {?*}\" \n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence) \n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = [(\"The\", \"DT\"), (\"small\", \"JJ\"), (\"red\", \"JJ\"),(\"flower\", \"NN\"), \n",
    "(\"flew\", \"VBD\"), (\"through\", \"IN\"),  (\"the\", \"DT\"), (\"window\", \"NN\")]\n",
    "grammar = \"NP: {?*}\" \n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence) \n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "all_cats = []\n",
    "for w in movie_reviews.categories():\n",
    "    all_cats.append(w.lower())\n",
    "print(all_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meteor threat set to blow away all volcanoes & twisters !\n",
      "summer is here again !\n",
      "this season could probably be the most ambitious = season this decade with hollywood churning out films like deep impact , = godzilla , the x-files , armageddon , the truman show , all of which has but = one main aim , to rock the box office .\n",
      "leading the pack this summer is = deep impact , one of the first few film releases from the = spielberg-katzenberg-geffen's dreamworks production company .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import sent_tokenize\n",
    "fields = movie_reviews.fileids()\n",
    "\n",
    "sample = movie_reviews.raw(\"pos/cv944_13521.txt\")\n",
    "\n",
    "token = sent_tokenize(sample)\n",
    "for lines in range(4):\n",
    "    print(token[lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "fields = movie_reviews.fileids()\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Are', 'you'),\n",
       " ('you', 'fully'),\n",
       " ('fully', 'aware'),\n",
       " ('aware', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'implications'),\n",
       " ('implications', 'of'),\n",
       " ('of', 'your'),\n",
       " ('your', 'action'),\n",
       " ('action', '?')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "word_data = \"Are you fully aware of the implications of your action?\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)  \t\n",
    "\n",
    "list(nltk.bigrams(nltk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sky is bright.\n",
      " \n",
      "Birds are gone.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdfName = 'C:/Users/Admin/AppData/Local/Programs/Python/Python37/skysahu.pdf'\n",
    "read_pdf = PyPDF2.PdfFileReader(pdfName)\n",
    "page = read_pdf.getPage(0)\n",
    "page_content = page.extractText()\n",
    "print(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page no- 0\n",
      "Sky is bright.\n",
      " \n",
      "Birds are gone.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Page no- 1\n",
      "Nests are empty.\n",
      " \n",
      "we can find these lines on http://www.google.com\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdfName = 'C:/Users/Admin/AppData/Local/Programs/Python/Python37/skysahu.pdf'\n",
    "read_pdf = PyPDF2.PdfFileReader(pdfName)\n",
    "\n",
    "for i in range(read_pdf.getNumPages()):\n",
    "    page = read_pdf.getPage(i)\n",
    "    print(\"Page no-\",read_pdf.getPageNumber(page))\n",
    "    page_content = page.extractText()\n",
    "    print(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sky is bright.\n",
      "Birds are gone.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nests are empty.\n",
      "we can find these lines on http://www.google.com\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def readtxt(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "print(readtxt('C:/Users/Admin/AppData/Local/Programs/Python/Python37/brightsahu.docx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "doc = docx.Document('C:/Users/Admin/AppData/Local/Programs/Python/Python37/brightsahu.docx')\n",
    "print(len(doc.paragraphs))\n",
    "\n",
    "print(doc.paragraphs[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"
     ]
    }
   ],
   "source": [
    "print(doc.paragraphs[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'title_detail', 'summary', 'summary_detail', 'links', 'link', 'id', 'guidislink', 'published', 'published_parsed'])\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "NewsFeed = feedparser.parse(\"https://economictimes.indiatimes.com/jobs/rssfeeds/107115.cms\")\n",
    "entry = NewsFeed.entries[1]\n",
    "\n",
    "print(entry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RSS posts : 15\n",
      "Post Title : Job offers drying up for IITians this placement season\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "NewsFeed = feedparser.parse(\"https://economictimes.indiatimes.com/jobs/rssfeeds/107115.cms\")\n",
    "\n",
    "print('Number of RSS posts :', len(NewsFeed.entries))\n",
    "entry = NewsFeed.entries[1]\n",
    "print('Post Title :',entry.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-24T08:02:56+05:30\n",
      "******\n",
      "<a href=\"https://economictimes.indiatimes.com/jobs/job-offers-drying-up-for-iitians-this-placement-season/articleshow/75334086.cms\"><img width=\"100\" height=\"75\" border=\"0\" hspace=\"10\" align=\"left\" src=\"https://img.etimg.com/photo/75334086.cms\" /></a>IITs are also reaching out to alumni (most of who are either startup founders or hold senior positions in India Inc) to help students get jobs, including for those whose offers have been revoked. Some are even taking the hiring process online as 30% of students have not been placed yet.\n",
      "------News Link--------\n",
      "https://economictimes.indiatimes.com/jobs/job-offers-drying-up-for-iitians-this-placement-season/articleshow/75334086.cms\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "NewsFeed = feedparser.parse(\"https://economictimes.indiatimes.com/jobs/rssfeeds/107115.cms\")\n",
    "\n",
    "entry = NewsFeed.entries[1]\n",
    "\n",
    "print(entry.published)\n",
    "print(\"******\")\n",
    "print(entry.summary)\n",
    "print(\"------News Link--------\")\n",
    "print(entry.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ** Sentiment with one word **\n",
      "\n",
      "{'contains(good)': True, 'contains(progress)': True, 'contains(luck)': False}\n",
      "\n",
      "*** Sentiment with bigrams ***\n",
      "\n",
      "{'contains(Regular - fit)': False, 'contains(fit - fine)': False}\n",
      "\n",
      "**Sentiment with Negative words**\n",
      "\n",
      "['Lack', 'of', 'good', 'health', 'can', 'not', 'bring_NEG', 'success_NEG', 'to_NEG', 'students_NEG']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.sentiment.sentiment_analyzer \n",
    "\n",
    "# Analysing for single words\n",
    "def OneWord():\n",
    "    positive_words = ['good', 'progress', 'luck']\n",
    "    text = 'Hard Work brings progress and good luck.'.split()                 \n",
    "    analysis = nltk.sentiment.util.extract_unigram_feats(text, positive_words) \n",
    "    print(' ** Sentiment with one word **\\n')\n",
    "    print(analysis) \n",
    "\n",
    "# Analysing for a pair of words\t\n",
    "def WithBigrams(): \n",
    "\tword_sets = [('Regular', 'fit'), ('fit', 'fine')] \n",
    "\ttext = 'Regular excercise makes you fit and fine'.split() \n",
    "\tanalysis = nltk.sentiment.util.extract_bigram_feats(text, word_sets) \n",
    "\tprint('\\n*** Sentiment with bigrams ***\\n') \n",
    "\tprint(analysis)\n",
    "    \n",
    "# Analysing the negation words. \n",
    "def NegativeWord():\n",
    "\ttext = 'Lack of good health can not bring success to students'.split() \n",
    "\tanalysis = nltk.sentiment.util.mark_negation(text) \n",
    "\tprint('\\n**Sentiment with Negative words**\\n')\n",
    "\tprint(analysis) \n",
    "    \n",
    "OneWord()\n",
    "WithBigrams() \n",
    "NegativeWord() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. search result found anywhere in the string\n",
      "2. Match with beginning of string\n",
      "3. No match with match if not beginning\n",
      "4. search as match\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "if  re.search(\"tor\", \"Tutorial\"):\n",
    "        print(\"1. search result found anywhere in the string\")\n",
    "        \n",
    "if re.match(\"Tut\", \"Tutorial\"):\n",
    "         print(\"2. Match with beginning of string\" )\n",
    "         \n",
    "if not re.match(\"tor\", \"Tutorial\"):\n",
    "        print(\"3. No match with match if not beginning\" )\n",
    "\n",
    "\n",
    "        \n",
    "# Search as Match\n",
    "        \n",
    "if  not re.search(\"^tor\", \"Tutorial\"):\n",
    "        print(\"4. search as match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hlelo, You suhold rceah the fnsiih lnie.\n",
      "Hlelo, You suhold rcaeh the finish lnie.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def replace(t):\n",
    "    inner_word = list(t.group(2))\n",
    "    random.shuffle(inner_word)\n",
    "    return t.group(1) + \"\".join(inner_word) + t.group(3)\n",
    "text = \"Hello, You should reach the finish line.\"\n",
    "print(re.sub(r\"(\\w)(\\w+)(\\w)\", replace, text))\n",
    "\n",
    "print(re.sub(r\"(\\w)(\\w+)(\\w)\", replace, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In late summer 1945, guests are gathered for the wedding reception of',\n",
       " \"Don Vito Corleone's daughter Connie (Talia Shire) and Carlo Rizzi\",\n",
       " '(Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia',\n",
       " 'family, is known to friends and associates as Godfather. He and Tom',\n",
       " 'Hagen (Robert Duvall), the Corleone family lawyer, are hearing',\n",
       " 'requests for favors because, according to Italian tradition, no',\n",
       " \"Sicilian can refuse a request on his daughter's wedding day. One of\",\n",
       " 'the men who asks the Don for a favor is Amerigo Bonasera, a successful',\n",
       " 'mortician and acquaintance of the Don, whose daughter was brutally',\n",
       " 'beaten by two young men because she refused their advances; the men',\n",
       " 'received minimal punishment from the presiding judge. The Don is',\n",
       " \"disappointed in Bonasera, who'd avoided most contact with the Don due\",\n",
       " \"to Corleone's nefarious business dealings. The Don's wife is godmother\",\n",
       " \"to Bonasera's shamed daughter, a relationship the Don uses to extract\",\n",
       " 'new loyalty from the undertaker. The Don agrees to have his men punish',\n",
       " 'the young men responsible (in a non-lethal manner) in return for',\n",
       " 'future service if necessary.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import parawrap\n",
    "\n",
    "text = \"In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone's daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia family, is known to friends and associates as Godfather. He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors because, according to Italian tradition, no Sicilian can refuse a request on his daughter's wedding day. One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician and acquaintance of the Don, whose daughter was brutally beaten by two young men because she refused their advances; the men received minimal punishment from the presiding judge. The Don is disappointed in Bonasera, who'd avoided most contact with the Don due to Corleone's nefarious business dealings. The Don's wife is godmother to Bonasera's shamed daughter, a relationship the Don uses to extract new loyalty from the undertaker. The Don agrees to have his men punish the young men responsible (in a non-lethal manner) in return for future service if necessary.\"\n",
    "\n",
    "parawrap.wrap(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'late ', 'summe', 'r', '1945,', 'guest', 's are', 'gathe', 'red', 'for', 'the w', 'eddin', 'g rec', 'eptio', 'n of', 'Don', 'Vito ', 'Corle', \"one's\", 'daugh', 'ter C', 'onnie', '(Tali', 'a Shi', 're)', 'and', 'Carlo', 'Rizzi', '(Gian', 'ni Ru', 'sso).', 'Vito ', '(Marl', 'on Br', 'ando)', ', the', 'head', 'of', 'the C', 'orleo', 'ne', 'Mafia', 'famil', 'y, is', 'known', 'to fr', 'iends', 'and a', 'ssoci', 'ates', 'as Go', 'dfath', 'er.', 'He', 'and', 'Tom', 'Hagen', '(Robe', 'rt Du', 'vall)', ', the', 'Corle', 'one f', 'amily', 'lawye', 'r,', 'are h', 'earin', 'g req', 'uests', 'for f', 'avors', 'becau', 'se, a', 'ccord', 'ing', 'to It', 'alian', 'tradi', 'tion,', 'no Si', 'cilia', 'n can', 'refus', 'e a r', 'eques', 't on', 'his d', 'aught', \"er's \", 'weddi', 'ng', 'day.', 'One', 'of', 'the', 'men', 'who', 'asks', 'the', 'Don', 'for a', 'favor', 'is Am', 'erigo', 'Bonas', 'era,', 'a suc', 'cessf', 'ul mo', 'rtici', 'an', 'and a', 'cquai', 'ntanc', 'e of', 'the', 'Don,', 'whose', 'daugh', 'ter', 'was b', 'rutal', 'ly be', 'aten', 'by', 'two', 'young', 'men b', 'ecaus', 'e she', 'refus', 'ed', 'their', 'advan', 'ces;', 'the', 'men r', 'eceiv', 'ed mi', 'nimal', 'punis', 'hment', 'from', 'the p', 'resid', 'ing j', 'udge.', 'The', 'Don', 'is di', 'sappo', 'inted', 'in Bo', 'naser', 'a,', \"who'd\", 'avoid', 'ed', 'most ', 'conta', 'ct', 'with', 'the', 'Don', 'due', 'to Co', 'rleon', \"e's n\", 'efari', 'ous b', 'usine', 'ss de', 'aling', 's.', 'The', \"Don's\", 'wife', 'is go', 'dmoth', 'er to', 'Bonas', \"era's\", 'shame', 'd dau', 'ghter', ', a r', 'elati', 'onshi', 'p the', 'Don', 'uses', 'to ex', 'tract', 'new l', 'oyalt', 'y', 'from', 'the u', 'ndert', 'aker.', 'The', 'Don a', 'grees', 'to', 'have', 'his', 'men p', 'unish', 'the', 'young', 'men r', 'espon', 'sible', '(in a', 'non-l', 'ethal', 'manne', 'r) in', 'retur', 'n for', 'futur', 'e ser', 'vice', 'if ne', 'cessa', 'ry.']\n"
     ]
    }
   ],
   "source": [
    "import parawrap\n",
    "\n",
    "text = \"In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone's daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia family, is known to friends and associates as Godfather. He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors because, according to Italian tradition, no Sicilian can refuse a request on his daughter's wedding day. One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician and acquaintance of the Don, whose daughter was brutally beaten by two young men because she refused their advances; the men received minimal punishment from the presiding judge. The Don is disappointed in Bonasera, who'd avoided most contact with the Don due to Corleone's nefarious business dealings. The Don's wife is godmother to Bonasera's shamed daughter, a relationship the Don uses to extract new loyalty from the undertaker. The Don agrees to have his men punish the young men responsible (in a non-lethal manner) in return for future service if necessary.\"\n",
    "\n",
    "print(parawrap.wrap(text,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs\n",
      "<zip object at 0x000002282589D788>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "sample = gutenberg.raw(\"blake-poems.txt\")\n",
    "\n",
    "token = word_tokenize(sample)\n",
    "wlist = []\n",
    "\n",
    "for i in range(50):\n",
    "    wlist.append(token[i])\n",
    "\n",
    "wordfreq = [wlist.count(w) for w in wlist]\n",
    "print(\"Pairs\\n\" + str(zip(token, wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sky', 1),\n",
       " ('is', 1),\n",
       " ('bright', 1),\n",
       " ('.', 2),\n",
       " ('Birds', 1),\n",
       " ('are', 2),\n",
       " ('gone', 1),\n",
       " ('.', 2),\n",
       " ('Nests', 1),\n",
       " ('are', 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "sample = gutenberg.raw(\"C:/Users/Admin/AppData/Local/Programs/Python/Python37/sahu.txt\")\n",
    "\n",
    "token = word_tokenize(sample)\n",
    "wlist = []\n",
    "\n",
    "for i in range(10):\n",
    "    wlist.append(token[i])\n",
    "\n",
    "wordfreq = [wlist.count(w) for w in wlist]\n",
    "list(zip(token, wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          may might  must  will \n",
      "hobbies   131    22    83   264 \n",
      "romance    11    51    45    43 \n",
      "  humor     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (genre, word)\n",
    "          for genre in brown.categories()\n",
    "          for word in brown.words(categories=genre))\n",
    "categories = ['hobbies', 'romance','humor']\n",
    "searchwords = [ 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=categories, samples=searchwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician and acquaintance of the Don, whose daughter was brutally beaten by two young men because sherefused their advances; the men received minimal punishment from the presiding judge.\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "text =\"In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones \" + \\\n",
    "       \"daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando),\"  + \\\n",
    "       \"the head of the Corleone Mafia family, is known to friends and associates as Godfather. \"  + \\\n",
    "       \"He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors \"  + \\\n",
    "       \"because, according to Italian tradition, no Sicilian can refuse a request on his daughter's wedding \" + \\\n",
    "       \" day. One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician \"  + \\\n",
    "       \"and acquaintance of the Don, whose daughter was brutally beaten by two young men because she\"  + \\\n",
    "       \"refused their advances; the men received minimal punishment from the presiding judge. \" + \\\n",
    "       \"The Don is disappointed in Bonasera, who'd avoided most contact with the Don due to Corleone's\" + \\\n",
    "       \"nefarious business dealings. The Don's wife is godmother to Bonasera's shamed daughter, \" + \\\n",
    "       \"a relationship the Don uses to extract new loyalty from the undertaker. The Don agrees \" + \\\n",
    "       \"to have his men punish the young men responsible (in a non-lethal manner) in return for \" + \\\n",
    "        \"future service if necessary.\"\n",
    "          \n",
    "print(summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corleone\n",
      "men\n",
      "corleones daughter\n",
      "summer\n",
      "new\n",
      "wedding\n",
      "vito\n",
      "family\n",
      "robert\n",
      "hagen\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "text = \"In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones \" + \\\n",
    "       \"daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando),\"  + \\\n",
    "       \"the head of the Corleone Mafia family, is known to friends and associates as Godfather. \"  + \\\n",
    "       \"He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors \"  + \\\n",
    "       \"because, according to Italian tradition, no Sicilian can refuse a request on his daughter's wedding \" + \\\n",
    "       \" day. One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician \"  + \\\n",
    "       \"and acquaintance of the Don, whose daughter was brutally beaten by two young men because she\"  + \\\n",
    "       \"refused their advances; the men received minimal punishment from the presiding judge. \" + \\\n",
    "       \"The Don is disappointed in Bonasera, who'd avoided most contact with the Don due to Corleone's\" + \\\n",
    "       \"nefarious business dealings. The Don's wife is godmother to Bonasera's shamed daughter, \" + \\\n",
    "       \"a relationship the Don uses to extract new loyalty from the undertaker. The Don agrees \" + \\\n",
    "       \"to have his men punish the young men responsible (in a non-lethal manner) in return for \" + \\\n",
    "        \"future service if necessary.\"\n",
    "\n",
    "print(keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***PorterStemmer****\n",
      "\n",
      "Actual: Aging  || Stem: age\n",
      "Actual: head  || Stem: head\n",
      "Actual: of  || Stem: of\n",
      "Actual: famous  || Stem: famou\n",
      "Actual: crime  || Stem: crime\n",
      "Actual: family  || Stem: famili\n",
      "Actual: decides  || Stem: decid\n",
      "Actual: to  || Stem: to\n",
      "Actual: transfer  || Stem: transfer\n",
      "Actual: his  || Stem: hi\n",
      "Actual: position  || Stem: posit\n",
      "Actual: to  || Stem: to\n",
      "Actual: one  || Stem: one\n",
      "Actual: of  || Stem: of\n",
      "Actual: his  || Stem: hi\n",
      "Actual: subalterns  || Stem: subaltern\n",
      "\n",
      "***LancasterStemmer****\n",
      "\n",
      "Actual: Aging  || Stem: ag\n",
      "Actual: head  || Stem: head\n",
      "Actual: of  || Stem: of\n",
      "Actual: famous  || Stem: fam\n",
      "Actual: crime  || Stem: crim\n",
      "Actual: family  || Stem: famy\n",
      "Actual: decides  || Stem: decid\n",
      "Actual: to  || Stem: to\n",
      "Actual: transfer  || Stem: transf\n",
      "Actual: his  || Stem: his\n",
      "Actual: position  || Stem: posit\n",
      "Actual: to  || Stem: to\n",
      "Actual: one  || Stem: on\n",
      "Actual: of  || Stem: of\n",
      "Actual: his  || Stem: his\n",
      "Actual: subalterns  || Stem: subaltern\n",
      "\n",
      "***SnowballStemmer****\n",
      "\n",
      "Actual: Aging  || Stem: age\n",
      "Actual: head  || Stem: head\n",
      "Actual: of  || Stem: of\n",
      "Actual: famous  || Stem: famous\n",
      "Actual: crime  || Stem: crime\n",
      "Actual: family  || Stem: famili\n",
      "Actual: decides  || Stem: decid\n",
      "Actual: to  || Stem: to\n",
      "Actual: transfer  || Stem: transfer\n",
      "Actual: his  || Stem: his\n",
      "Actual: position  || Stem: posit\n",
      "Actual: to  || Stem: to\n",
      "Actual: one  || Stem: one\n",
      "Actual: of  || Stem: of\n",
      "Actual: his  || Stem: his\n",
      "Actual: subalterns  || Stem: subaltern\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lanca_stemmer = LancasterStemmer()\n",
    "sb_stemmer = SnowballStemmer(\"english\",)\n",
    "\n",
    "word_data = \"Aging head of famous crime family decides to transfer his position to one of his subalterns\" \n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "print('***PorterStemmer****\\n')\n",
    "for w_port in nltk_tokens:\n",
    "    print(\"Actual: %s  || Stem: %s\"  % (w_port,porter_stemmer.stem(w_port)))\n",
    "\n",
    "print('\\n***LancasterStemmer****\\n')\n",
    "for w_lanca in nltk_tokens:\n",
    "    print(\"Actual: %s  || Stem: %s\"  % (w_lanca,lanca_stemmer.stem(w_lanca)))\n",
    "print('\\n***SnowballStemmer****\\n')\n",
    "\n",
    "for w_snow in nltk_tokens:\n",
    "    print(\"Actual: %s  || Stem: %s\"  % (w_snow,sb_stemmer.stem(w_snow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main web Address:  https://www.tutorialspoint.com\n",
      "The protocol:  https\n",
      "The doman name:  www.tutorialspoint\n",
      "The TLD:  com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"The web address is https://www.tutorialspoint.com\"\n",
    "\n",
    "# Taking \"://\" and \".\" to separate the groups \n",
    "result = re.search('([\\w.-]+)://([\\w.-]+)\\.([\\w.-]+)', text)\n",
    "if result :\n",
    "    print(\"The main web Address: \",result.group())\n",
    "    print(\"The protocol: \",result.group(1))\n",
    "    print(\"The doman name: \",result.group(2) )\n",
    "    print(\"The TLD: \",result.group(3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
